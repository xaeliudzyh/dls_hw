{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"align: center;\"><img align=center src=\"https://drive.google.com/uc?export=view&id=1aS4vX-ucDKBmZmZMrBwgjl_DvLAadX2C\" width=900/></p>"
      ],
      "metadata": {
        "id": "tUn4MDRPvXGr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co-eo4BURtaT"
      },
      "source": [
        "<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n",
        "\n",
        "<h1 style=\"text-align: center;\"><b>Семинар. Регуляризация в линейных алгоритмах</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRoRKVq43yvE"
      },
      "source": [
        "В этом семинаре мы поговорим об $L_1$- и $L_2$-регуляризации линейной регрессии, а также об ElasticNet. Реализуем линейную регресиию с Lasso-регуляризацией и сравним реализацию с моделью из sklearn.\n",
        "\n",
        "Зачастую модель машинного обучения обучается на зашумлённых данных, то есть данных с ошибками и случайными отклонениями. Модель машинного обучения, которая обучается на минимизацию функции потерь, не может автоматически понять, где в датасете реальные важные закономерности, а где -- ошибки и случайные совпадения. Нам бы хотелось заставить модель **не переобучаться** под такие проблемы в данных и вычленять только неслучаные закономерности. Иначе, идеально обучившись на датасете с шумом, мы можем получить плохой результат на тестовых данных.\n",
        "\n",
        "На практике переобучение проявляется в излишней сложности модели. **Регуляризация** -- это метод борьбы с переобучением, который штрафует модель за излишнюю сложность сложность, что позволяет строить более простые (и потому стабильные) зависимости.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yBNj-arF6FO"
      },
      "source": [
        "Еще одной проблемой, специфичной для линейных моделей, является **мультиколлинеарность**. Как разбиралось на лекции, если оптимальных решений задачи минимизации оказывается бесконечно много, то коэффициенты модели могут принимать какие угодно огромные значения. **Вывод**: необходимо *ограничить величину* коэффициентов модели.\n",
        "\n",
        "Для линейной модели дополнительные ограничения на веса выполняют роль регуляризации.\n",
        "\n",
        "Функция потерь для линейной регрессии с регуляризацией выглядит следующим образом:\n",
        "$$L(\\mathbf{w}) = \\frac{1}{\\ell}\\sum_{i=1}^{\\ell}(\\langle \\mathbf{w} , x^i \\rangle - y^i)^2 + R(\\tilde{ \\mathbf{w}}) \\rightarrow \\min_{w}$$\n",
        "где\n",
        "* $x^i = (1, x^i_1, \\ldots, x^i_n)$ --- вектор признаков $i$-ого объекта;\n",
        "* $y^i \\in\\mathbb{R}$ --- правильный ответ на $i$-ом объекте;\n",
        "* $\\mathbf{w} = (w_0, w_1, \\ldots, w_n)$ --- вектор весов ($w_0$ --- свободный член).\n",
        "* $\\tilde {\\mathbf{w}} =  (w_1, \\ldots, w_n)$ --- вектор весов без свободного члена.\n",
        "\n",
        "Последнее слагаемое определяет вид регуляризации.\n",
        "* $L_1$-регуляризация (LASSO, least absolute shrinkage and selection operator), регуляризационное слагаемое равно $$R( \\mathbf{\\tilde w}) = \\lambda|| \\mathbf{\\tilde {w}}||_1 = \\lambda (|w_1| + \\ldots + |w_n|);$$\n",
        "* $L_2$-регуляризация (Ridge), регуляризационное слагаемое равно $$R(\\tilde{ \\mathbf{w}}) = \\lambda||\\tilde{ \\mathbf{w}}||_2^2 = \\lambda(|w_1|^2 + \\ldots + |w_n|^2);$$\n",
        "* ElasticNet -- комбинация двух предыдущих, регуляризационное слагаемое равно $$R(\\tilde{ \\mathbf{w}}) = \\alpha ||\\tilde{\\mathbf{w}}||_1+ \\beta ||\\tilde {\\mathbf{w}}||_2^2.$$\n",
        "\n",
        "Обратите внимание, что во всех случаях **коэффициент $w_0$ не участвует в сумме!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjjtruRfRtaW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.linalg as sla\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i8WNBT5sePU"
      },
      "source": [
        "## Lasso-регрессия\n",
        "\n",
        "В LASSO-регрессии мы штрафуем модель **на сумму модулей всех ее весов**.\n",
        "\n",
        "**Лосс:** $$L(\\mathbf{w}) = \\frac{1}{\\ell} ||X\\mathbf{w} - \\mathbf{y}||^2_2 + \\lambda ||\\tilde{\\mathbf{w}}||_1,$$ где $\\lambda$ -- гиперпараметр, отвечающий за степень регуляризации.\n",
        "\n",
        "В привычном понимании:\n",
        "\n",
        "**Лосс:** $$L(\\mathbf{w}) = \\frac{1}{\\ell}\\sum_{i=1}^{\\ell}\\left(\\sum_{j=0}^{n} x_{ij}w_j - y_i\\right)^2 + \\lambda\\sum_{j=1}^{n}|w_j|$$\n",
        "\n",
        "\n",
        "\n",
        "**Градиент:**\n",
        "$$\n",
        "\\frac{\\partial{L}}{\\partial{\\mathbf{w}}}\n",
        "= \\frac{2}{\\ell}\\cdot X^T(X\\mathbf{w} - \\mathbf{y}) + \\lambda (0, \\mathrm{sign}(w_1), \\ldots, \\mathrm{sign}(w_n))^T.\n",
        "$$\n",
        "\n",
        "Будем считать, что $|\\cdot|$ -- дифференцируемая функция, ее производной является $sign(\\cdot)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETKNC1HxlgXd"
      },
      "outputs": [],
      "source": [
        "def soft_sign(x, eps=1e-7):\n",
        "    if abs(x) > eps:\n",
        "        return np.sign(x)\n",
        "    return x / eps\n",
        "\n",
        "np_soft_sign = np.vectorize(soft_sign)\n",
        "\n",
        "\n",
        "class MyLassoRegression(object):\n",
        "    def __init__(self, C=1):\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.C = C\n",
        "\n",
        "    def regularization_term(self, weights):\n",
        "        signs =  # YOUR CODE. Calculate soft signs of weights  # [n+1, 1]\n",
        "        signs[0] = 0  # Не нужно регуляризовывать по свободному члену\n",
        "        return signs\n",
        "\n",
        "    def grad(self, X, y, weights):\n",
        "        y_pred = (X @ weights)  # [l, 1]\n",
        "\n",
        "        basic_term =  # YOUR CODE. Calulate basic term of loss  # [n+1, 1]\n",
        "\n",
        "        regularization_term = self.regularization_term(weights)  # [n+1, 1]\n",
        "\n",
        "        return basic_term + self.C * regularization_term  # [n+1, 1]\n",
        "\n",
        "\n",
        "    def fit(self, X, y, max_iter=100, lr=0.1):\n",
        "        # Принимает на вход X, y и вычисляет веса по данной выборке.\n",
        "        # Не забудьте про фиктивный признак, равный 1!\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        assert len(y.shape) == 1 and len(X.shape) == 2\n",
        "        assert X.shape[0] == y.shape[0]\n",
        "\n",
        "        y = y[:, np.newaxis]\n",
        "\n",
        "        l, n = X.shape\n",
        "\n",
        "        # Добавляем признак из единиц\n",
        "        X_train = np.hstack([np.ones([l, 1]), X])  # [ell, n + 1]\n",
        "\n",
        "        # Инициализируем веса\n",
        "        weights = np.random.randn(n + 1, 1)\n",
        "\n",
        "        losses = []\n",
        "\n",
        "        for iter_num in range(max_iter):\n",
        "            # calculate grad\n",
        "            grad = self.grad(X_train, y, weights)\n",
        "            # update weights\n",
        "            weights -= grad * lr / ((iter_num + 1) ** 0.5)\n",
        "\n",
        "            # calculate loss\n",
        "            loss = np.mean((X_train @ weights - y) ** 2) + self.C * np.sum(np.abs(weights[1:]))\n",
        "            losses.append(loss)\n",
        "\n",
        "        # assign coef, intersept\n",
        "        self.coef_ = weights[1:]\n",
        "        self.intercept_ = weights[0]\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        y_pred = X @ self.coef_ + self.intercept_\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Php_z5Yv2Iku"
      },
      "source": [
        "Протестируем нашу функцию на одномерной регрессии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqo5nBBp2Wzy"
      },
      "outputs": [],
      "source": [
        "def linear_expression(x):\n",
        "    return 5 * x + 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnWKgFrL1zHF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "objects_num = 50\n",
        "X = np.linspace(-5, 5, objects_num)\n",
        "y = linear_expression(X) + np.random.randn(objects_num) * 5\n",
        "\n",
        "# выделим половину объектов на тест\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oUXohPi2SsZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(X, linear_expression(X), label='real', c='g')\n",
        "plt.scatter(X_train, y_train, label='train', c='b')\n",
        "plt.scatter(X_test, y_test, label='test', c='orange')\n",
        "\n",
        "plt.title(\"Generated dataset\")\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BV7XBQ782ow-"
      },
      "outputs": [],
      "source": [
        "regressor = MyLassoRegression()\n",
        "\n",
        "losses = regressor.fit(X_train[:, np.newaxis], y_train)\n",
        "\n",
        "predictions = regressor.predict(X_test[:, np.newaxis])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTYW_rKw4jun"
      },
      "outputs": [],
      "source": [
        "regressor.coef_, regressor.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCTi2zP54fEj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(losses, label='loss')\n",
        "plt.legend(fontsize=14)\n",
        "plt.xlabel('iter', fontsize=14)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMKUq3GC2c-F"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(X, linear_expression(X), label='real', c='g')\n",
        "plt.plot(X, regressor.predict(X[:, np.newaxis]))\n",
        "\n",
        "plt.scatter(X_train, y_train, label='train', c='b')\n",
        "plt.scatter(X_test, y_test, label='test', c='orange')\n",
        "\n",
        "plt.title(\"Generated dataset\")\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwgtaI8algCR"
      },
      "source": [
        "## Загрузка датасета\n",
        "Загрузим набор данных, с которым мы будем работать. В библиотеке scikit-learn есть множество тренировочных наборов данных для освоения и проверки методов машинного обучения. Мы будем работать с датасетом Diabetes. Этот датасет содержит данные о развитии диабета у пациента. Всего в датасете 10 признаков.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=19oN1ydcPobFRKc6Bx6Bf8tLVmI4Kh4g4\" width=\"400\">\n",
        "\n",
        "Стандартные наборы данных в scikit-learn находятся в модуле sklearn.datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIRwIhBWlgCU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_diabetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuxTXnfjlgCh"
      },
      "outputs": [],
      "source": [
        "data = load_diabetes()\n",
        "\n",
        "print(data['DESCR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os_ObK2nlgCs"
      },
      "source": [
        "### Выделение данных\n",
        "\n",
        "Выделим матрицу объекты-признаки в переменную $X$, правильные ответы --- в переменную $y$. Используем библиотеку pandas. Для отображения информации о наборе данных используем функцию pd.describe, которая отображает полезные статистики из набора: средние значения признаков, минимум, максимум, медиану и др."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PZXmxzylgCv"
      },
      "outputs": [],
      "source": [
        "X = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
        "y = data['target']\n",
        "\n",
        "X.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrYEfpne3pRl"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBMX_Qx032ca"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled =  # YOUR CODE. Fit and apply scaler\n",
        "X_test_scaled =  # YOUR CODE. Apply scaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE. Verify that scaler worked OK"
      ],
      "metadata": {
        "id": "CRiuFACyTpau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdQ8q3B54PDQ"
      },
      "outputs": [],
      "source": [
        "model = MyLassoRegression(C=0.001)\n",
        "losses = model.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8InsmEhD6Y3e"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(losses, label='loss')\n",
        "plt.legend(fontsize=14)\n",
        "plt.xlabel('iter', fontsize=14)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wefA3bz24GH6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 8))\n",
        "plt.bar(X.columns, model.coef_.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2ZnL6Lr6SE5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "y_train_prediction = model.predict(X_train_scaled)\n",
        "y_test_prediction = model.predict(X_test_scaled)\n",
        "\n",
        "print(f'Train MSE: {mean_squared_error(y_train, y_train_prediction)}')\n",
        "print(f'Test MSE: {mean_squared_error(y_test, y_test_prediction)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDf-d8NK8Ge9"
      },
      "outputs": [],
      "source": [
        "model.coef_, model.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEiaIt-Y62mJ"
      },
      "source": [
        "## Величина весов в зависимости от коэффициента регуляризации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uGJXX5X6oTm"
      },
      "outputs": [],
      "source": [
        "reg_coefs = np.linspace(10, 100, 50)\n",
        "\n",
        "weights = np.empty((len(X.columns), 0))\n",
        "for C in reg_coefs:\n",
        "    lasso_regressor = MyLassoRegression(C=C)\n",
        "    lasso_regressor.fit(X_train_scaled, y_train, lr=0.05, max_iter=5000)\n",
        "    weights = np.hstack((weights, lasso_regressor.coef_.reshape(-1, 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NByRDwXt3WjB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "for weights_for_feature, column_name in zip(weights, X.columns):\n",
        "    plt.plot(reg_coefs, weights_for_feature, label=f'weights of feature {column_name}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xYg6aFK_wZN"
      },
      "source": [
        "Построим такие же графики для реализации из sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjronqkT_vEM"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi5jb6VG_qDD"
      },
      "outputs": [],
      "source": [
        "reg_coefs = np.linspace(5, 50, 50)\n",
        "\n",
        "weights = np.empty((len(X.columns), 0))\n",
        "for C in reg_coefs:\n",
        "    lasso_regressor = Lasso(alpha=C)\n",
        "    lasso_regressor.fit(X_train_scaled, y_train)\n",
        "    weights = np.hstack((weights, lasso_regressor.coef_.reshape(-1, 1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "for weights_for_feature, column_name in zip(weights, X.columns):\n",
        "    plt.plot(reg_coefs, weights_for_feature, label=f'weights of feature {column_name}')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ElcqB1HUsWb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "1c1fa0f7-95a3-4cba-9c88-b9e55a314a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c88e5fd527f9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mweights_for_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_coefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_for_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'weights of feature {column_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDY8EJmPsrCy"
      },
      "source": [
        "# 2.1. Ridge регрессия (L2-регуляризация)\n",
        "\n",
        "В ridge мы штрафуем модель также на сумму квадратов всех ее весов, таким образом:\n",
        "\n",
        "**Лосс:**\n",
        "$$L(\\mathbf{w}) = ||X\\mathbf{w} - \\mathbf{y}||^2_2 + \\lambda||\\mathbf{\\tilde{w}}||^2_2 = \\sum_{i=1}^{\\ell}\\left(\\sum_{j=0}^{n} x_{ij}w_j - y_i\\right)^2 + \\lambda\\sum_{j=1}^{n}w_j^2,$$\n",
        "где $\\lambda$ --- гиперпараметр, отвечающий за степень регуляризации.\n",
        "\n",
        "Что стоит сказать про значения признаков? Они должны быть стандартизованы для одинаковых штрафов относительно друг друга! (используется связка с `sklearn.preprocessing.StandardScaler`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0FPG3-bAYKh"
      },
      "source": [
        "Сравнение с графиками для Ridge-регрессии:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6SFVdYc_8f6"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "reg_coefs = np.linspace(1, 10000, 100)\n",
        "\n",
        "weights = np.empty((len(X.columns), 0))\n",
        "for C in reg_coefs:\n",
        "    ridge_regressor = Ridge(C)\n",
        "    ridge_regressor.fit(X_train_scaled, y_train)\n",
        "    weights = np.hstack((weights, ridge_regressor.coef_.reshape(-1, 1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "for weights_for_feature, column_name in zip(weights, X.columns):\n",
        "    plt.plot(reg_coefs, weights_for_feature, label=f'weights of feature {column_name}')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wd7adQ0WU3qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ker3ZqpRjhIA"
      },
      "source": [
        "Различия между $L_1$- и $L_2$-регуляризациями:\n",
        "\n",
        "- Lasso **сложнее обучать** из-за отсутствия аналитического решения\n",
        "- В Lasso появляется **зануление весов** для некоторых признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-l3qGrmhlyi"
      },
      "source": [
        "## Для самостоятельного изучения: реализация Ridge-регрессии\n",
        "\n",
        "Аналогично предыдущим заданиям нужно рассчитать значение градиента $\\displaystyle\\frac{\\partial{L}}{\\partial{\\mathbf{w}}}$.\n",
        "\n",
        "**Градиент:**\n",
        "$$\n",
        "\\frac{\\partial{L}}{\\partial{\\mathbf{w}}}\n",
        "= \\frac{2}{\\ell}\\cdot X^T(X\\mathbf{w} - \\mathbf{y}) + 2\\lambda \\cdot (0, w_1, \\ldots, w_n)^T.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R39nSHvmIRqp"
      },
      "outputs": [],
      "source": [
        "class MyRidgeRegression(object):\n",
        "    def __init__(self, C=1):\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.C = C\n",
        "\n",
        "    def regularization_term(self, weights):\n",
        "        outp = 2 * weights.copy()\n",
        "        outp[0] = 0  # Не нужно регуляризовывать по свободному члену\n",
        "        return outp\n",
        "\n",
        "    def grad(self, X, y, weights):\n",
        "        y_pred = (X @ weights)  # [ell, 1]\n",
        "\n",
        "        basic_term = 2. / X.shape[0] * (X.T @ (y_pred - y))\n",
        "\n",
        "        regularization_term = self.regularization_term(weights)\n",
        "\n",
        "        return basic_term + self.C * regularization_term\n",
        "\n",
        "\n",
        "    def fit(self, X, y, max_iter=100, lr=0.1):\n",
        "        # Принимает на вход X, y и вычисляет веса по данной выборке.\n",
        "        # Не забудьте про фиктивный признак, равный 1!\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        assert len(y.shape) == 1 and len(X.shape) == 2\n",
        "        assert X.shape[0] == y.shape[0]\n",
        "\n",
        "        y = y[:, np.newaxis]\n",
        "\n",
        "        l, n = X.shape\n",
        "\n",
        "        # Добавляем признак из единиц\n",
        "        X_train = np.hstack([np.ones([l, 1]), X])  # [l, n+1]\n",
        "\n",
        "        # Инициализируем веса\n",
        "        weights = np.random.randn(n+1, 1)\n",
        "\n",
        "        losses = []\n",
        "\n",
        "        for iter_num in range(max_iter):\n",
        "            # calculate grad\n",
        "            grad = self.grad(X_train, y, weights)\n",
        "            # update weights\n",
        "            weights -= grad * lr / ((iter_num + 1) ** 0.5)\n",
        "\n",
        "            # calculate loss\n",
        "            loss = np.mean((X_train @ weights - y) ** 2) + self.C * np.sum(np.abs(weights[1:]))\n",
        "            losses.append(loss)\n",
        "\n",
        "        # assign coef, intersept\n",
        "        self.coef_ = weights[1:]\n",
        "        self.intercept_ = weights[0]\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        y_pred = X @ self.coef_ + self.intercept_\n",
        "\n",
        "        return y_pred"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}